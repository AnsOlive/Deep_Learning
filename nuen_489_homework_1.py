# -*- coding: utf-8 -*-
"""NUEN_489_Homework_1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qe5i2a1SRnHVDYl8bab7RVyvuU1sB4Bw

# **Problem 1: Linear Regression**

Instuction: In this exercise, you will develop a linear regression model to predict a tree’s volume based on its girth and height. Data will be provided for this purpose.

Just importing all packages needed for both prolems- I didn't like that the drive was imported later.

Also please note to run this from the beginning and clear history afterward. Since I used similar variables for 1 and 2 these often conflist when restarting or testing certian cells. This script should work on initialization and a first run perfectly fine.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.optimize as opt
from matplotlib.colors import LogNorm
from mpl_toolkits.mplot3d import axes3d, Axes3D
import statsmodels.api as sm
from google.colab import drive

"""**Complete the problems 1.1 - 1.4. Decompose your code into different sessions for clarity**

## Problem 1.1: Visualizing the Data

Instruction: Access the dataset through the statsmodels package and store it in a pandas
dataframe. Begin by visualizing the data in a 3D plot, with the girth and height
as the x and y axes, and the volume as the z-axis.

All this was given.
"""

trees = sm.datasets.get_rdataset('trees').data
df = trees.round(3)
print("First few rows of the dataset:")
print(df.head())

"""This was given as well."""

y = df['Volume'].to_numpy()
X = df[['Girth', 'Height']].to_numpy()
m = y.size

# X.shape

# Print out some data points
print('First 10 examples from the dataset: ')
for i in range(0, 10):
    print('x = {}, y = {}'.format(X[i], y[i]))

trees.describe()

"""This creates a basic 3D plot of the data using matplotlib using girth as X, height as y, and volume as z."""

fig = plt.figure()
ax = fig.add_subplot(projection='3d')
ax.scatter(df['Girth'], df['Height'], df['Volume'])
ax.set_xlabel('Girth')
ax.set_ylabel('Height')
ax.set_zlabel('Volume')
ax.set_title("Trees Dataset")
plt.show()

"""## Problem 1.2: Defining Loss Function and Performing Gradient Descent

Instruction: Define the loss function for linear regression and apply gradient descent to find
the optimal parameters. Apply the gradient descent algorithm according to the equation discussed in the lecture.

I normalized the X matrix by column so gradient descent will converge properly. For some reason it diverged even though I do not consider the scale to be 'large' possibly partially due to the high number of iterations. I also initialized the X-intercept column.
"""

X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_scaled = (X - X_mean) / X_std
X_intercept = np.hstack((np.ones((m, 1)), X_scaled))

"""I defined the gradient descent as a function for the equation given, which needs the X matrix, y vector, the learning rate, and a number of iterations to try to see convergence for.

This takes the number of points as m, the and number of datasets as n. It then creates an array of zeros for each n and initializes a loss list to keep track for each iteration.

It then loops through for the number of iterations specified and predicts the value and sees how far off it is from the actual value as error.

This error is used for the loss equation as well as m for the number of entries. The graident is also computed by using the error and m, as well as the X matrix. Omega is then also updated using the equation given.

It finally returns the optimizes parameter and the list of losses for each iteration.
"""

def gradient_descent(X, y, alpha, num_iters):

    m, n = X.shape
    omega = np.zeros(n)
    loss_history = []

    for i in range(num_iters):
        predictions = X.dot(omega)
        error = predictions - y

        loss = (1 / (2 * m)) * np.sum(error ** 2)
        gradient = (1 / m) * X.T.dot(error)
        omega = omega - alpha * gradient
        loss_history.append(loss)

    return omega, loss_history

"""## Problem 1.3: Exploring the Effect of Learning Rate

Instructions: Evaluate the impact of different learning rates (0.3, 0.1, 0.03, 0.01) on the model.
Plot the history of loss for each learning rate.

This iterates through the specified learning rates to solve the problem for each. I chose 500 iterations to ensure show convergence for all learning rates, including a closer convergence for the semilog graph.

The loss histories are needed to plot and the final parameters to compare to the closed-form solution.
"""

learning_rates = [0.3, 0.1, 0.03, 0.01]
num_iters = 500
loss_histories = {}
gd_params = {}

for alpha in learning_rates:
    omega_gd, loss_history = gradient_descent(X_intercept, y, alpha, num_iters)
    loss_histories[alpha] = loss_history
    gd_params[alpha] = omega_gd
    print(f"\nFinal parameters for learning rate {alpha}: {omega_gd}")

"""I added the figure to show the plot for each of the learning rates and how the loss converged over a number of iterations. I included a semilog plot so the lower values were easily readable as well."""

plt.figure()
for alpha, loss_history in loss_histories.items():
    plt.plot(loss_history, label=f'alpha = {alpha}')
plt.xlabel('Iteration')
plt.ylabel('Loss (MSE)')
plt.title('Linear Scale Learning Rates')
plt.legend()
plt.grid()
plt.show()

plt.figure()
for alpha, loss_history in loss_histories.items():
    plt.semilogy(loss_history, label=f'alpha = {alpha}')
plt.xlabel('Iteration')
plt.ylabel('Loss (MSE) [Log Scale]')
plt.title('Semilog Plot of Loss Convergence for Different Learning Rates')
plt.legend()
plt.grid()
plt.show()

"""## Problem 1.4: Comparing GD Results with Closed-Form Solution

Instructions: Compare the results obtained from gradient descent with those from the closed-
form solution of linear regression.

I computed the closed-form solution and compared it to the most rapidly converging solution to see what percentage difference is obtained. Until alpha = 0.01 the percentage difference for most values is so small it is nearly negligable.
"""

omega_closed = np.linalg.inv(X_intercept.T.dot(X_intercept)).dot(X_intercept.T).dot(y)
print("\nClosed-form solution parameters: ")
print(omega_closed)

for i in learning_rates:
  print(f"\nGradient Descent percent difference from closed-form for alpha = {i}: ")
  print(((abs(gd_params[i] - omega_closed))/((gd_params[i] + omega_closed)/2)) * 100)

"""# Problem 2: Logistic Regression

Instructions: Develop a logistic regression model, with and without regularization, to predict the likelihood of microchips from a manufacturing plant passing quality
assurance (QA).

**Load data**

The data is stored in Google drive, this step mount the google drive and load the data through numpy.

**Note, you need to change the path of the data file based on your case**

**Complete Problems 2.1-2.5, decompose your code into different sessions for clarity.**

## Problem 2.1: Visualizing the Data

Instructions: Load the ’logistic.txt’ data file and visualize it as demonstrated in the lecture.
Use different markers to represent positive (y = 1) and negative (y = 0) out-
comes.

All this was given except for the correct path. I shortened it a bit because my permissions were acting up. I had to manually connect it from the Google Colab sidebar.
"""

data = np.loadtxt('/content/drive/MyDrive/489_Homeworks/489_Homeowrk_1/logistic_homework.txt', delimiter=',')
X = data[:, 0:2]
y = data[:, 2]

"""I simply plotted the data given by adapting some code from the given 'plot_decision_boundary' function."""

plt.figure()
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='+', c='b', label='Pass')
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', c='y', label='Fail')
plt.xlabel('Microchip Test 1')
plt.ylabel('Microchip Test 2')
plt.title('Microchip QA Data')
plt.legend()
plt.show()

"""## Problem 2.2: Defining Loss Function and Performing Gradient Descent

Instructions: Define the loss function for logistic regression and apply gradient descent to find
the optimal parameters. Choose an appropriate learning rate for the problem and report its value.

I like to define all needed functions in one place per problem so I also included the given functions here.

This first section calls the map_features function to create the necessary polynomial to draw the line for the descision model and stores the updated X in X_mapped.
"""

def map_feature(x1, x2):
    degree = 6

    x1 = x1.reshape((x1.size, 1))
    x2 = x2.reshape((x2.size, 1))
    result = np.ones(x1[:, 0].shape)

    for i in range(1, degree + 1):
        for j in range(0, i + 1):
            result = np.c_[result, (x1**(i-j)) * (x2**j)]

    return result

X_mapped = map_feature(X[:, 0], X[:, 1])

"""This just defines the given plotDecisionBoundary function to be called later."""

def plotDecisionBoundary(omega, X, y):
    """
    Plots the data points and overlays the decision boundary computed using
    the parameter vector omega. X here is the original (unmapped) 2D feature array.
    """
    plt.figure()
    pos = np.where(y == 1)
    neg = np.where(y == 0)
    plt.scatter(X[pos, 0], X[pos, 1], marker='+', c='b', label='y=1')
    plt.scatter(X[neg, 0], X[neg, 1], marker='o', c='r', label='y=0')
    plt.xlabel('Microchip Test 1')
    plt.ylabel('Microchip Test 2')
    plt.title('Decision Boundary')

    # Create a grid to evaluate the decision boundary.
    u = np.linspace(np.min(X[:, 0]) - 1, np.max(X[:, 0]) + 1, 50)
    v = np.linspace(np.min(X[:, 1]) - 1, np.max(X[:, 1]) + 1, 50)
    z = np.zeros((len(u), len(v)))

    # Evaluate z = mapped_features.dot(omega) over the grid.
    for i in range(len(u)):
        for j in range(len(v)):
            mapped = map_feature(np.array([u[i]]), np.array([v[j]]))
            z[i, j] = mapped.dot(omega)[0]
    z = z.T  # Transpose for correct orientation.

    # Plot the contour where z = 0 (i.e. decision boundary at h(x)=0.5)
    plt.contour(u, v, z, levels=[0], colors='g')
    plt.legend()
    plt.show()

"""This calls a gradient descent function for the overall logistic problem. It calls for a gradient and loss for each iteration and returns the optimized parameters and the loss as the number of iterations increase.

This is also split on wether or not there is a regularization strength, so I have a singular function for both this section and 2.5 since it runs regardless and defaults to no regularization strength.
"""

def hypothesis(X, omega):
    h = 1 / (1 + np.exp(-(X.dot(omega))))
    return h


def losses(omega, X, y, lam):

    m = len(y)
    h = hypothesis(X, omega)
    loss = - (1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))

    if lam != 0:
        reg = (lam / (2 * m)) * np.sum(omega[1:] ** 2)
        loss += reg

    return loss


def gradient(omega, X, y, lam):

    m = len(y)
    h = hypothesis(X, omega)
    grad = (1 / m) * X.T.dot(h - y)

    if lam != 0:
      grad[1:] = grad[1:] + (lam / m) * omega[1:]

    return grad


def gradient_descent_logistic(X, y, omega_init, alpha, iterations, lam=0):

    omega = omega_init.copy()
    loss_history = []

    for i in range(iterations):
        loss = losses(omega, X, y, lam)
        grad = gradient(omega, X, y, lam)
        loss_history.append(loss)
        omega = omega - alpha * grad

    return omega, loss_history

"""This creates initial omegas and applies the earlier functions to create a graph for the non-regularizes gradient descent. I selected a learning rate of 0.1 since it should converge fairly quickly while maintaining accuracy. I selected 10000 iterations since it looks like that is a number that shows evident convergence."""

initial_omega = np.zeros(X_mapped.shape[1])
learning_rate = 0.1
iterations = 10000

omega_gd, loss_hist = gradient_descent_logistic(X_mapped, y, initial_omega, learning_rate, iterations)
print("Gradient Descent (No Regularization) parameters:")
print(omega_gd)

plt.figure()
plt.plot(loss_hist)
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Loss History (No Regularization)')
plt.show()

"""## Problem 2.3: Comparing with opt.fmin bfgs Optimizer

Instruction: Use the opt.fmin bfgs optimizer to find the optimized parameters and compare
them with the results from gradient descent.

This runs the losses function using BFGS using no regularization by haveing no regularization strength. It then compares how accurate they are by percentage difference.

I've concluded that the large values from the non-regularized BFGS are because there are no penalties. If there is a different reason or an error in my code please tell me but that is the reason I found when looking at why that data differed so much from all the others.

I also incuded the test case of the tiniest amount of regularization strength to test this, which shows results more usual to the other values. I will use a very large amount of regularization strength for the next section to see how that effects things.
"""

lam = 0.0
omega_bfgs = opt.fmin_bfgs(losses, initial_omega, fprime=gradient, args=(X_mapped, y, lam), disp=True)
print("\nBFGS Optimizer (No Regularization) parameters:")
print(omega_bfgs)

print("\nGradient Descent percent difference from the BFGS values to our calculated values: ")
percent_diff_list = abs((omega_bfgs - omega_gd)/((omega_bfgs + omega_gd)/2)) * 100
print(percent_diff_list)
print(f"\nThe average percent difference from the BFGS values to our calculated values is: {np.average(percent_diff_list)}")

# lam=0.01
# omega_bfgs = opt.fmin_bfgs(losses, initial_omega, fprime=gradient, args=(X_mapped, y, lam), disp=True)
# print("\nBFGS Optimizer (Tiny Regularization) parameters:")
# print(omega_bfgs)

# print("\nGradient Descent percent difference from the BFGS values to our calculated values: ")
# percent_diff_list = abs((omega_bfgs - omega_gd)/((omega_bfgs + omega_gd)/2)) * 100
# print(percent_diff_list)
# print(f"\nThe average percent difference from the BFGS values to our calculated values is: {np.average(percent_diff_list)}")

"""## Problem 2.4: Plotting the Decision Boundary

Instruction: Use the provided function plotDecisionBoundary.py to illustrate the decision
boundary of your classifier (note: the boundary will not be ”correct”, just plot
it).

Condiering QA can often be very inconsistant, which the data here shows, the boundary both having valid ones outside the border and invalid ones inside makes sense. I did all the setup in previous steps due to how I like formatting. Keep in mind this is only the non-regularized model.
"""

plotDecisionBoundary(omega_gd, X, y)

"""## Problem 2.5: Feature Mapping and Regularization (optional for NUEN 489 students)

I opted initially not to do this- my worklad was large this week but I got too caught up in it. It was too much fun! I'm thinking of going back and polishing it even more- I'll definitely tinker with using the logistic regression because I think it's particularly fun/interesting.

There is nothing really new in this setion, it is just reformatted functions from the last few sections because I decided to already incorporate the regularization in the initial function.
"""

lam = 1.0
omega_bfgs_reg = opt.fmin_bfgs(losses, initial_omega, fprime=gradient, args=(X_mapped, y, lam), disp=True)
print("BFGS Optimizer (Regularization, lambda = 1) parameters:")
print(omega_bfgs_reg)

omega_gd_reg, loss_hist_reg = gradient_descent_logistic(X_mapped, y, initial_omega, learning_rate, iterations, lam)
print("\nGradient Descent (Regularization, lambda = 1) parameters:")
print(omega_gd_reg)

print("\nGradient Descent percent difference from the BFGS values to our calculated values: ")
percent_diff_list = abs((omega_bfgs_reg - omega_gd_reg)/((omega_bfgs_reg + omega_gd_reg)/2)) * 100
print(percent_diff_list)
print(f"\nThe average percent difference from the BFGS values to our calculated values is: {np.average(percent_diff_list)}")

plt.figure()
plt.plot(loss_hist_reg)
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Loss History (Regularization, lambda = 1)')
plt.show()

plotDecisionBoundary(omega_gd_reg, X, y)